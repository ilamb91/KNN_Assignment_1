{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fbcfcd-8e70-497a-846b-3c7dae3cfc60",
   "metadata": {},
   "source": [
    "# Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f7b59-9bd3-4a13-bbc8-47fbee51e72d",
   "metadata": {},
   "source": [
    "A1\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple and widely used supervised machine learning algorithm for classification and regression tasks. It's a type of instance-based learning or lazy learning where the algorithm makes predictions by finding the K training examples (data points) in its training dataset that are closest in distance to the input data point and then assigns a label or value based on the majority class or average of those K neighbors.\n",
    "\n",
    "Here's how the KNN algorithm works:\n",
    "\n",
    "1. Choose the number of neighbors (K): K is a user-defined hyperparameter that specifies how many neighboring data points to consider when making predictions. A common choice is to use an odd value for K to avoid ties when determining the majority class.\n",
    "\n",
    "2. Calculate distances: For a given input data point (the one you want to classify or predict), calculate the distance between that point and all other data points in the training dataset. Common distance metrics include Euclidean distance, Manhattan distance, or Minkowski distance.\n",
    "\n",
    "3. Find the K-nearest neighbors: Select the K training examples with the shortest distances to the input data point.\n",
    "\n",
    "4. Make predictions:\n",
    "   - For classification: Assign the class label that is most common among the K nearest neighbors. This can be determined by a majority vote among the classes of the neighbors.\n",
    "   - For regression: Take the average (or weighted average) of the target values of the K nearest neighbors to predict the target value of the input data point.\n",
    "\n",
    "5. Output the prediction.\n",
    "\n",
    "KNN is a simple algorithm and is often used as a baseline in machine learning experiments. However, it has some limitations and considerations:\n",
    "- It can be computationally expensive for large datasets since it requires calculating distances between the input point and all training points.\n",
    "- The choice of K can significantly affect the algorithm's performance. A smaller K may lead to noise sensitivity, while a larger K may result in a loss of decision boundary details.\n",
    "- The algorithm's performance may degrade if the feature scales are not standardized, as features with larger scales may dominate the distance calculations.\n",
    "\n",
    "Despite its simplicity, KNN can be effective in many cases, especially when the dataset is not very large and the decision boundaries are relatively simple. It is also a non-parametric method, meaning it doesn't make strong assumptions about the underlying data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fcc860-d81c-4b93-b71f-d8a2e7105e56",
   "metadata": {},
   "source": [
    "# Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff7cee-2bed-4e7c-952f-16596a3b16fd",
   "metadata": {},
   "source": [
    "A2\n",
    "\n",
    "Choosing the right value of K in the K-Nearest Neighbors (KNN) algorithm is a crucial step, as it can significantly impact the model's performance. The choice of K depends on the specific dataset and problem you are working on. Here are some common methods to select an appropriate value for K:\n",
    "\n",
    "1. **Odd vs. Even K:**\n",
    "   - It's often a good practice to choose an odd value for K to avoid ties when voting for the majority class in classification problems. Ties can lead to ambiguous predictions.\n",
    "\n",
    "2. **Rule of Thumb:**\n",
    "   - A common starting point is to use the square root of the number of data points in your training dataset as a reasonable K value. For example, if you have 100 data points, you might start with K = âˆš100 = 10.\n",
    "\n",
    "3. **Cross-Validation:**\n",
    "   - Use techniques like cross-validation (e.g., k-fold cross-validation) to evaluate the model's performance for different values of K. This helps you find the K that provides the best trade-off between bias and variance. You can try a range of K values and select the one that yields the best cross-validation performance.\n",
    "\n",
    "4. **Grid Search:**\n",
    "   - Perform a grid search over a predefined range of K values to find the optimal K. This involves training and evaluating the model for each K in the specified range and selecting the one with the best performance based on a chosen evaluation metric (e.g., accuracy for classification or mean squared error for regression).\n",
    "\n",
    "5. **Domain Knowledge:**\n",
    "   - Consider any domain-specific knowledge or prior information you have about the problem. Some problems may have inherent characteristics that suggest a suitable range for K. For example, if you know that there are only two classes and they are well-separated, a smaller K might be appropriate.\n",
    "\n",
    "6. **Experimentation:**\n",
    "   - Try different K values and observe the model's performance on a validation dataset or through cross-validation. Plotting the performance metrics (e.g., accuracy) as a function of K can help you visualize the impact of K on your model's performance.\n",
    "\n",
    "7. **Bias-Variance Trade-off:**\n",
    "   - Keep in mind the bias-variance trade-off. Smaller values of K tend to result in a more complex and flexible model (lower bias but higher variance), while larger values of K lead to a smoother decision boundary (higher bias but lower variance). Choose K that strikes a balance between underfitting and overfitting.\n",
    "\n",
    "8. **Ensemble Methods:**\n",
    "   - Consider using ensemble methods like Random Forest or Gradient Boosting, which can mitigate the impact of choosing an incorrect K by aggregating multiple KNN models with different K values.\n",
    "\n",
    "In practice, it's important to experiment with different K values and evaluate your model's performance using appropriate validation techniques. The choice of K should be driven by the specific characteristics of your dataset and the goals of your machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c333085-ad20-4121-8315-8e604dd817f9",
   "metadata": {},
   "source": [
    "# Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4991edcb-b98f-49a1-8a21-78a5026e322c",
   "metadata": {},
   "source": [
    "A3\n",
    "\n",
    "K-Nearest Neighbors (KNN) can be used for both classification and regression tasks, and the primary difference between KNN classifier and KNN regressor lies in their respective purposes and how they make predictions:\n",
    "\n",
    "1. **KNN Classifier:**\n",
    "   - **Purpose:** KNN classifier is used for classification tasks, where the goal is to assign a class label to a given input data point.\n",
    "   - **Prediction:** For a KNN classifier, when making predictions for a new data point, it identifies the K nearest neighbors from the training data and assigns the class label that is most common among those neighbors. The majority vote or mode of the K neighbors' classes determines the predicted class label.\n",
    "   - **Output:** The output of a KNN classifier is a discrete class label, and it works well for problems like image classification, spam detection, and sentiment analysis.\n",
    "\n",
    "2. **KNN Regressor:**\n",
    "   - **Purpose:** KNN regressor is used for regression tasks, where the goal is to predict a continuous numerical value (i.e., a real number) for a given input data point.\n",
    "   - **Prediction:** For a KNN regressor, when making predictions for a new data point, it identifies the K nearest neighbors from the training data and computes the average (or weighted average) of their target values. This average value becomes the predicted output for the input data point.\n",
    "   - **Output:** The output of a KNN regressor is a continuous numerical value, and it is suitable for problems like house price prediction, stock price forecasting, and any other task where the target variable is a real-valued quantity.\n",
    "\n",
    "In summary, the key difference is in the type of output that each variant of KNN provides. KNN classifier assigns discrete class labels based on the majority class among the nearest neighbors, while KNN regressor predicts a continuous numerical value based on the average of the target values of the nearest neighbors. The choice between KNN classifier and KNN regressor depends on the nature of your machine learning problem and the type of data you are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f03d6-2edf-4f7f-9149-35b88e6a618b",
   "metadata": {},
   "source": [
    "# Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd74e05-3778-4750-83e8-3b86ef24a0e0",
   "metadata": {},
   "source": [
    "A4\n",
    "\n",
    "To measure the performance of a K-Nearest Neighbors (KNN) model, you can use a variety of evaluation metrics depending on whether you are working on a classification or regression problem. Here are common performance metrics for KNN:\n",
    "\n",
    "**For KNN Classifier (Classification Problems):**\n",
    "\n",
    "1. **Accuracy:** Accuracy is one of the most straightforward metrics for classification. It measures the proportion of correctly classified instances out of the total instances in the test dataset. However, accuracy can be misleading in imbalanced datasets.\n",
    "\n",
    "2. **Precision:** Precision measures the proportion of true positive predictions (correctly predicted positive instances) out of all positive predictions made by the model. It's a good metric when the cost of false positives is high.\n",
    "\n",
    "3. **Recall (Sensitivity or True Positive Rate):** Recall measures the proportion of true positive predictions out of all actual positive instances. It's useful when you want to ensure that all positive cases are captured and the cost of false negatives is high.\n",
    "\n",
    "4. **F1-Score:** The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall, especially when you have imbalanced datasets.\n",
    "\n",
    "5. **Confusion Matrix:** A confusion matrix provides a detailed breakdown of true positives, true negatives, false positives, and false negatives, which can be used to calculate various performance metrics and gain insights into the model's behavior.\n",
    "\n",
    "6. **ROC Curve and AUC (Area Under the ROC Curve):** These metrics are useful for binary classification problems. ROC curves visualize the trade-off between true positive rate and false positive rate at various classification thresholds, while AUC quantifies the overall model performance.\n",
    "\n",
    "7. **Classification Report:** A classification report typically includes precision, recall, F1-score, and support for each class in a multi-class classification problem. It's a comprehensive summary of classification performance.\n",
    "\n",
    "**For KNN Regressor (Regression Problems):**\n",
    "\n",
    "1. **Mean Absolute Error (MAE):** MAE measures the average absolute difference between the predicted values and the true values. It provides a simple and interpretable measure of error.\n",
    "\n",
    "2. **Mean Squared Error (MSE):** MSE measures the average of the squared differences between predicted and true values. It penalizes larger errors more heavily than MAE.\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE):** RMSE is the square root of MSE and provides an error metric in the same unit as the target variable. It's sensitive to outliers.\n",
    "\n",
    "4. **R-squared (R2) Score:** R2 measures the proportion of the variance in the target variable that is explained by the model. It ranges from 0 to 1, with higher values indicating better model fit.\n",
    "\n",
    "5. **Mean Absolute Percentage Error (MAPE):** MAPE calculates the average percentage difference between predicted and true values. It's useful when you want to understand the error in percentage terms.\n",
    "\n",
    "6. **Residual Analysis:** Visual inspection of residual plots can provide insights into the model's performance, especially in regression tasks. Residual plots should ideally show a random scatter around zero.\n",
    "\n",
    "To assess the performance of your KNN model, you can choose one or more of these metrics depending on the specific problem and goals. Additionally, consider using cross-validation to get a more robust estimate of model performance and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbff025-61a2-4e87-8af3-c3c3f96b8393",
   "metadata": {},
   "source": [
    "# Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc23d9-ccb9-4068-8864-ed8ff29bad6f",
   "metadata": {},
   "source": [
    "A5\n",
    "\n",
    "The \"curse of dimensionality\" is a term used in machine learning and statistics to describe the challenges and issues that arise when dealing with high-dimensional feature spaces, particularly in algorithms like K-Nearest Neighbors (KNN). It refers to the exponential increase in data volume that occurs as the number of features or dimensions in a dataset grows. This phenomenon can have several negative effects on the performance and efficiency of KNN and other machine learning algorithms:\n",
    "\n",
    "1. **Increased Computational Complexity:** As the dimensionality of the feature space increases, the number of data points required to maintain the same level of data density also grows exponentially. This means that KNN needs to search through a much larger space to find the nearest neighbors, resulting in significantly increased computational time and memory requirements.\n",
    "\n",
    "2. **Sparsity of Data:** In high-dimensional spaces, data points tend to become more sparse, meaning that they are often far apart from each other. This sparsity makes it challenging to find meaningful and relevant neighbors, as many data points may not be close enough to provide useful information.\n",
    "\n",
    "3. **Distance Metrics Become Less Effective:** Common distance metrics like Euclidean distance may become less meaningful in high dimensions. In high-dimensional spaces, all data points tend to be approximately equidistant from each other, making it difficult to distinguish between similar and dissimilar data points.\n",
    "\n",
    "4. **Overfitting:** With a large number of dimensions and limited data, the risk of overfitting the model increases. KNN can easily memorize the training data and perform poorly on unseen data because it might end up capturing noise or outliers present in the high-dimensional space.\n",
    "\n",
    "5. **Increased Data Requirements:** To maintain the same level of statistical significance, you would need exponentially more data points as the dimensionality increases. Collecting and managing such large datasets can be impractical or expensive.\n",
    "\n",
    "6. **Curse of Sample Size:** The curse of dimensionality can lead to a situation where the effective sample size (i.e., the number of data points that are meaningfully different from each other) decreases, making it difficult to generalize from the data.\n",
    "\n",
    "To mitigate the curse of dimensionality in KNN and similar algorithms, practitioners often employ dimensionality reduction techniques such as Principal Component Analysis (PCA) or feature selection methods to reduce the number of irrelevant or redundant features. Additionally, choosing an appropriate distance metric that is less affected by high dimensionality (e.g., Mahalanobis distance) can be beneficial.\n",
    "\n",
    "In some cases, alternative machine learning algorithms that are less sensitive to high-dimensional spaces, such as linear models or tree-based methods, may be preferred over KNN when working with high-dimensional data. It's important to carefully consider the characteristics of your dataset and the specific requirements of your problem when choosing an algorithm and addressing the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb38bd-6b39-47e5-bfce-d98878ac9552",
   "metadata": {},
   "source": [
    "# Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759f0d1-168c-448f-a2e6-cca04c3bb2fc",
   "metadata": {},
   "source": [
    "A6.\n",
    "\n",
    "Handling missing values in the K-Nearest Neighbors (KNN) algorithm requires careful consideration, as missing data can significantly affect the performance of the algorithm. Here are some common approaches to deal with missing values in KNN:\n",
    "\n",
    "1. **Data Imputation:**\n",
    "   - One of the most straightforward approaches is to impute (fill in) missing values with estimated values. Common imputation methods include:\n",
    "     - Mean, median, or mode imputation: Replace missing values with the mean, median, or mode of the corresponding feature. This approach is simple but may not be suitable for all types of data.\n",
    "     - Regression imputation: Use a regression model (e.g., linear regression) to predict missing values based on other features.\n",
    "     - KNN imputation: Utilize KNN itself to impute missing values. For each missing value, find the K nearest neighbors (based on available features) and average or vote for the missing value.\n",
    "\n",
    "2. **Removing Rows with Missing Values:**\n",
    "   - If you have relatively few missing values, you can choose to remove rows with missing data. This approach may be appropriate when the proportion of missing values is small, and the remaining data is still representative of the overall dataset.\n",
    "\n",
    "3. **Feature Engineering:**\n",
    "   - Sometimes, the presence of missing values may carry meaningful information. You can create binary \"indicator\" variables to explicitly capture whether a value is missing or not for a particular feature. These indicator variables can be used as additional features in the KNN algorithm.\n",
    "\n",
    "4. **Advanced Imputation Techniques:**\n",
    "   - Consider using more advanced imputation techniques, such as k-Nearest Neighbors with imputation (KNNI), which combines KNN with imputation to estimate missing values more accurately.\n",
    "\n",
    "5. **KNN Weighted by Similarity:**\n",
    "   - In some cases, you may want to weight the contributions of neighboring points based on their similarity to the point with missing values. More similar neighbors can have a higher influence on imputed values.\n",
    "\n",
    "6. **Domain-Specific Imputation:**\n",
    "   - Depending on your domain knowledge and the nature of your data, you may devise domain-specific imputation strategies that are more appropriate for your problem.\n",
    "\n",
    "It's important to note that the choice of the imputation method should depend on the characteristics of your dataset, the nature of the missing data, and the goals of your analysis. Additionally, you should evaluate the impact of the imputation method on the performance of your KNN model using appropriate validation techniques, such as cross-validation, to ensure that it doesn't introduce bias or result in overly optimistic performance estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9aabac-a7ef-4ca0-8299-53bd01a1eb44",
   "metadata": {},
   "source": [
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d46d7-f3ff-44fb-b706-e62fbf15ee0e",
   "metadata": {},
   "source": [
    "A7.\n",
    "\n",
    "The choice between a K-Nearest Neighbors (KNN) classifier and a KNN regressor depends on the nature of the problem you are trying to solve, specifically whether it's a classification or regression problem. Here's a comparison and contrast of the performance of KNN classifier and KNN regressor, along with guidance on which one is better suited for different types of problems:\n",
    "\n",
    "**KNN Classifier:**\n",
    "\n",
    "- **Purpose:** KNN classifier is used for classification tasks, where the goal is to assign a class label to a given input data point.\n",
    "\n",
    "- **Output:** The output of a KNN classifier is a discrete class label, which represents the predicted category or class that the input data point belongs to.\n",
    "\n",
    "- **Performance Metrics:** KNN classifiers are evaluated using metrics such as accuracy, precision, recall, F1-score, and confusion matrix. These metrics assess how well the model correctly assigns class labels.\n",
    "\n",
    "- **Use Cases:** KNN classifiers are suitable for problems where the target variable is categorical or nominal. Common use cases include image classification, spam detection, sentiment analysis, and disease diagnosis.\n",
    "\n",
    "**KNN Regressor:**\n",
    "\n",
    "- **Purpose:** KNN regressor is used for regression tasks, where the goal is to predict a continuous numerical value (e.g., price, temperature, or sales) for a given input data point.\n",
    "\n",
    "- **Output:** The output of a KNN regressor is a continuous numeric value, which represents the predicted quantity or value associated with the input data point.\n",
    "\n",
    "- **Performance Metrics:** KNN regressors are evaluated using metrics such as mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), R-squared (R2), and residual analysis. These metrics assess the accuracy and goodness of fit of the model's predictions.\n",
    "\n",
    "- **Use Cases:** KNN regressors are suitable for problems where the target variable is continuous and numeric. Common use cases include house price prediction, stock price forecasting, demand forecasting, and any scenario where you need to estimate a numerical value.\n",
    "\n",
    "**Which One to Choose:**\n",
    "\n",
    "- **Classification Problems:** Use KNN classifier for problems where the target variable represents categories or classes. It's appropriate when the goal is to make discrete predictions, such as determining whether an email is spam or not.\n",
    "\n",
    "- **Regression Problems:** Use KNN regressor for problems where the target variable is numeric and continuous. KNN regression is suitable for predicting quantities, values, or measurements.\n",
    "\n",
    "- **Mixed Problems:** In some cases, a problem may involve both classification and regression aspects. You can use a hybrid approach or ensemble methods to combine both KNN classifier and KNN regressor for such problems.\n",
    "\n",
    "- **Considerations:** Keep in mind the scalability and computational cost of KNN, especially for large datasets. KNN regressors can be computationally expensive, and the choice of distance metric and value of K can impact the performance.\n",
    "\n",
    "In summary, the choice between KNN classifier and KNN regressor depends on the nature of the target variable in your problem. KNN classifier is appropriate for categorical outcomes, while KNN regressor is suitable for predicting continuous numeric values. Carefully consider your problem's characteristics and objectives when selecting the appropriate KNN variant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060c5ed-7761-45e2-8999-7d6de92a68dd",
   "metadata": {},
   "source": [
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec0fcb-7c61-45be-9100-54b52d26b72b",
   "metadata": {},
   "source": [
    "A8.\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple and intuitive machine learning algorithm that can be effective for both classification and regression tasks. However, it has its strengths and weaknesses, which should be considered when deciding to use it and during model development. Here are the strengths and weaknesses of KNN for classification and regression tasks and ways to address them:\n",
    "\n",
    "**Strengths of KNN:**\n",
    "\n",
    "**1. Simplicity:** KNN is easy to understand and implement. It doesn't require complex mathematical equations or assumptions about the data distribution.\n",
    "\n",
    "**2. Versatility:** KNN can be applied to both classification and regression problems, making it a versatile algorithm.\n",
    "\n",
    "**3. Non-parametric:** KNN is a non-parametric algorithm, meaning it doesn't make strong assumptions about the underlying data distribution. This makes it suitable for various types of data.\n",
    "\n",
    "**4. Local Patterns:** It can capture local patterns and decision boundaries effectively, especially when the data is not linearly separable.\n",
    "\n",
    "**5. No Training Phase:** KNN doesn't have a traditional training phase, which means it can adapt quickly to changes in the data.\n",
    "\n",
    "**Weaknesses of KNN:**\n",
    "\n",
    "**1. Computational Complexity:** KNN can be computationally expensive, especially with large datasets, as it requires calculating distances between the query point and all training points.\n",
    "\n",
    "**2. Sensitivity to Data Density:** KNN is sensitive to variations in data density. In areas with sparse data, it may struggle to find meaningful neighbors.\n",
    "\n",
    "**3. Hyperparameter Tuning:** The choice of the hyperparameter K (number of neighbors) can significantly impact model performance and may require tuning.\n",
    "\n",
    "**4. Scaling:** KNN is sensitive to the scaling of features. Features with larger scales can dominate the distance calculations. Standardization or normalization of features is often necessary.\n",
    "\n",
    "**5. Curse of Dimensionality:** In high-dimensional spaces, KNN may suffer from the curse of dimensionality, where distances become less meaningful, and data points appear equidistant.\n",
    "\n",
    "**6. Imbalanced Data:** KNN can be biased toward the majority class in imbalanced datasets, especially when using a small value of K.\n",
    "\n",
    "**7. No Model Interpretability:** KNN does not provide model interpretability or feature importance scores, which may be important for understanding the model's decision-making process.\n",
    "\n",
    "**Addressing the Weaknesses of KNN:**\n",
    "\n",
    "1. **Computational Complexity:** To address computational complexity, consider dimensionality reduction techniques (e.g., PCA), approximations, or using efficient data structures like KD-trees or ball trees.\n",
    "\n",
    "2. **Data Density:** Ensure that you have sufficient data in areas of interest or consider using modified distance metrics that give more weight to relevant features.\n",
    "\n",
    "3. **Hyperparameter Tuning:** Experiment with different values of K through cross-validation and grid search to find the optimal K value for your dataset.\n",
    "\n",
    "4. **Feature Scaling:** Standardize or normalize your features to ensure that all features contribute equally to distance calculations.\n",
    "\n",
    "5. **Curse of Dimensionality:** Use feature selection or dimensionality reduction techniques to reduce the number of irrelevant or redundant features.\n",
    "\n",
    "6. **Imbalanced Data:** Consider oversampling, undersampling, or using different distance weighting schemes to handle imbalanced data.\n",
    "\n",
    "7. **Model Interpretability:** If interpretability is crucial, consider using interpretable algorithms or techniques like LIME or SHAP to explain KNN's predictions.\n",
    "\n",
    "In summary, KNN is a flexible and simple algorithm that can be effective for various tasks, but it has specific challenges, especially related to computational complexity and sensitivity to data characteristics. Addressing these weaknesses through appropriate preprocessing, hyperparameter tuning, and data handling strategies can help improve the performance of KNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728069e-e3e7-493c-bef7-e0932cd77f6b",
   "metadata": {},
   "source": [
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7c85d-6104-4507-affc-86a602da3ce5",
   "metadata": {},
   "source": [
    "A9.\n",
    "\n",
    "Euclidean distance and Manhattan distance are two common distance metrics used in the K-Nearest Neighbors (KNN) algorithm to measure the similarity or dissimilarity between data points. They differ in how they calculate the distance between points in a multi-dimensional space:\n",
    "\n",
    "1. **Euclidean Distance:**\n",
    "   - Euclidean distance is also known as L2 distance.\n",
    "   - It calculates the straight-line or \"as-the-crow-flies\" distance between two points in a Euclidean space (i.e., a space with coordinates, like the Cartesian plane).\n",
    "   - In two dimensions (2D), the Euclidean distance between two points (x1, y1) and (x2, y2) is calculated as:\n",
    "     \\[ \\text{Euclidean Distance} = \\sqrt{(x2 - x1)^2 + (y2 - y1)^2} \\]\n",
    "   - In n-dimensional space, the Euclidean distance between two points with coordinates (x1, y1, ..., xn) and (x2, y2, ..., xn) is calculated as:\n",
    "     \\[ \\text{Euclidean Distance} = \\sqrt{\\sum_{i=1}^{n} (x2_i - x1_i)^2} \\]\n",
    "\n",
    "2. **Manhattan Distance:**\n",
    "   - Manhattan distance is also known as L1 distance or city block distance.\n",
    "   - It calculates the distance by summing the absolute differences between the coordinates of two points along each dimension.\n",
    "   - In two dimensions (2D), the Manhattan distance between two points (x1, y1) and (x2, y2) is calculated as:\n",
    "     \\[ \\text{Manhattan Distance} = |x2 - x1| + |y2 - y1| \\]\n",
    "   - In n-dimensional space, the Manhattan distance between two points with coordinates (x1, y1, ..., xn) and (x2, y2, ..., xn) is calculated as:\n",
    "     \\[ \\text{Manhattan Distance} = \\sum_{i=1}^{n} |x2_i - x1_i| \\]\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "1. **Sensitivity to Directions:**\n",
    "   - Euclidean distance considers the direct, shortest path between two points, making it sensitive to the diagonal direction between points.\n",
    "   - Manhattan distance only considers horizontal and vertical movements (like navigating a city grid), making it insensitive to diagonal movements.\n",
    "\n",
    "2. **Formulas:**\n",
    "   - Euclidean distance uses the square root of the sum of squared differences between coordinates.\n",
    "   - Manhattan distance uses the sum of absolute differences between coordinates.\n",
    "\n",
    "3. **Use Cases:**\n",
    "   - Euclidean distance is appropriate when the underlying space is isotropic (uniform in all directions) and when you want to measure \"as-the-crow-flies\" distance.\n",
    "   - Manhattan distance is often used when movement can only occur along axes (e.g., movement in a city grid) or when you want to avoid overemphasizing the effects of outliers.\n",
    "\n",
    "In the context of KNN, the choice between Euclidean and Manhattan distance (or other distance metrics) depends on the characteristics of the data and the problem you are trying to solve. Experimentation with different distance metrics is often recommended to determine which one works best for your specific task and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c03584-7097-4d26-84be-37652c7e522d",
   "metadata": {},
   "source": [
    "# Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfde14-28c3-4a31-8d6e-169010cd79a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
